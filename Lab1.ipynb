{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0ae182f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install transformers, AutoTokenizer, torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "190f9fe4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11e6a57a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 25000, Test examples: 25000\n"
          ]
        }
      ],
      "source": [
        "# 1. Load a text dataset (IMDB movie reviews — raw text for LM)\n",
        "dataset = load_dataset(\"imdb\", split=\"train\")\n",
        "print(f\"Number of examples in dataset: {len(dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "4bb52a4a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Initialize a tokenizer (DistilGPT-2 — smaller/faster, same style as GPT-2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # DistilGPT-2 doesn't have a pad token by default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cd45b9f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 25000/25000 [00:03<00:00, 7811.57 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:02<00:00, 8694.28 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[40, 26399, 314, 3001, 327, 47269, 20958, 12, 56, 23304, 3913, 422, 616, 2008, 3650, 780, 286, 477, 262, 10386]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. Tokenize the dataset efficiently using `.map` with batched processing\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], return_special_tokens_mask=False)\n",
        "\n",
        "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "# The dataset now has columns like 'input_ids' and 'attention_mask'\n",
        "\n",
        "print(tokenized_ds[0][\"input_ids\"][:20])  # print first 20 token IDs of first example for sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe849645",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 25000/25000 [00:02<00:00, 11098.53 examples/s]\n",
            "Map: 100%|██████████| 25000/25000 [00:02<00:00, 11503.17 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train LM sequences: 51879, Val LM sequences: 51065\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. Slice into training sequences of fixed length\n",
        "# For language model training, often we concatenate all texts then split into blocks of e.g. 128 or 512 tokens.\n",
        "block_size = 128\n",
        "\n",
        "def _flatten(seqs):\n",
        "    \"\"\"Flatten to list of ints; works when elements are lists or numpy arrays.\"\"\"\n",
        "    out = []\n",
        "    for s in seqs:\n",
        "        out.extend(s.tolist() if hasattr(s, \"tolist\") else list(s))\n",
        "    return out\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate each field (flatten so we get token IDs, not list-of-arrays)\n",
        "    concatenated_inputs = _flatten(examples[\"input_ids\"])\n",
        "    concatenated_masks = _flatten(examples[\"attention_mask\"])\n",
        "\n",
        "    total_len = (len(concatenated_inputs) // block_size) * block_size\n",
        "    concatenated_inputs = concatenated_inputs[:total_len]\n",
        "    concatenated_masks = concatenated_masks[:total_len]\n",
        "\n",
        "    # Split into chunks\n",
        "    result_input_ids = [concatenated_inputs[i:i+block_size] for i in range(0, total_len, block_size)]\n",
        "    result_masks = [concatenated_masks[i:i+block_size] for i in range(0, total_len, block_size)]\n",
        "\n",
        "    return {\"input_ids\": result_input_ids, \"attention_mask\": result_masks}\n",
        "\n",
        "\n",
        "# remove_columns: drop \"label\" (and any other cols) so output has only chunk rows.\n",
        "batch_size_map = 1000\n",
        "lm_ds = tokenized_ds.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=batch_size_map,\n",
        "    remove_columns=tokenized_ds.column_names,\n",
        ")\n",
        "lm_ds_val = tokenized_ds_val.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=batch_size_map,\n",
        "    remove_columns=tokenized_ds_val.column_names,\n",
        ")\n",
        "print(f\"Train LM sequences: {len(lm_ds)}, Val LM sequences: {len(lm_ds_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5f9cca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Create a DataLoader for the tokenized, grouped dataset\n",
        "# We'll use a custom collate to dynamically pad sequences (though all are same length here by construction)\n",
        "def collate_fn(batch):\n",
        "    # Since our sequences are fixed length after grouping, we might just stack them.\n",
        "    # If they weren't fixed, we could use tokenizer.pad to pad to max length in batch.\n",
        "    input_ids = torch.tensor([example[\"input_ids\"] for example in batch], dtype=torch.long)\n",
        "    # For language modeling, labels are the input_ids shifted by one, but \n",
        "    # Transformers' CausalLM models usually handle that internally if we provide labels = input_ids.\n",
        "    return {\"input_ids\": input_ids, \"labels\": input_ids.clone()}\n",
        "\n",
        "train_loader = DataLoader(lm_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5edc38fd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch shapes: torch.Size([8, 128]) torch.Size([8, 128])\n"
          ]
        }
      ],
      "source": [
        "# 6. Iterate through a couple of batches to see that it works\n",
        "for batch in train_loader:\n",
        "    print(batch[\"input_ids\"].shape, batch[\"labels\"].shape)\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
